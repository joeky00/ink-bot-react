{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZB/hY6Ol+hNJepw0DYUs3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "536911b18093489bb600c4f9055d1461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e68f8fb9a32647b2826f5682974d94ce",
              "IPY_MODEL_a1e2dacf5cc54ba09fd401c9e32eaf7a",
              "IPY_MODEL_a5db64b8992f4f52885c2efb137431ab"
            ],
            "layout": "IPY_MODEL_16db819607214e0c9602f28f0f04f6e2"
          }
        },
        "e68f8fb9a32647b2826f5682974d94ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfb4810abefe40aea7869cdb7c45a0b0",
            "placeholder": "​",
            "style": "IPY_MODEL_5cc77cf955da4a8f8f821dba62c52290",
            "value": "Generating train split: "
          }
        },
        "a1e2dacf5cc54ba09fd401c9e32eaf7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_304fa4ccdc464a9096199810d427eb78",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0959b46a68b64637a0f092dcca613f0c",
            "value": 1
          }
        },
        "a5db64b8992f4f52885c2efb137431ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2efdd00a12994706a90bf4e3b46b5b0f",
            "placeholder": "​",
            "style": "IPY_MODEL_9b90a42fd1984b03a86ab57a041937b3",
            "value": " 1000/0 [00:00&lt;00:00, 25013.14 examples/s]"
          }
        },
        "16db819607214e0c9602f28f0f04f6e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb4810abefe40aea7869cdb7c45a0b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc77cf955da4a8f8f821dba62c52290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "304fa4ccdc464a9096199810d427eb78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0959b46a68b64637a0f092dcca613f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2efdd00a12994706a90bf4e3b46b5b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b90a42fd1984b03a86ab57a041937b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4f2f0fda66143e08d9df731792737de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fb72e842ec24ddb834cee250a6e3a52",
              "IPY_MODEL_ca6cb43f8582489c9d54f18b1baa2342",
              "IPY_MODEL_eefdfc24b8914da499bdb29fa84c981b"
            ],
            "layout": "IPY_MODEL_b556264ba9254ab1a4a2f7a9cb554fb3"
          }
        },
        "9fb72e842ec24ddb834cee250a6e3a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73a25d9061fe47c4aaed762921c1a3a1",
            "placeholder": "​",
            "style": "IPY_MODEL_813c87e2ad8b44868ae97aeae0a75f1a",
            "value": "Map: 100%"
          }
        },
        "ca6cb43f8582489c9d54f18b1baa2342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb00f5aec24e4532abcbab621f8994c1",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87bb6c820e3a4fe3827b4f50ec914efe",
            "value": 1000
          }
        },
        "eefdfc24b8914da499bdb29fa84c981b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99cedcbfd2c9444a83d72492ba9cdf1a",
            "placeholder": "​",
            "style": "IPY_MODEL_af41dd6cd28241f89a07d594aa2abfac",
            "value": " 1000/1000 [00:00&lt;00:00, 1922.50 examples/s]"
          }
        },
        "b556264ba9254ab1a4a2f7a9cb554fb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a25d9061fe47c4aaed762921c1a3a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813c87e2ad8b44868ae97aeae0a75f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb00f5aec24e4532abcbab621f8994c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87bb6c820e3a4fe3827b4f50ec914efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99cedcbfd2c9444a83d72492ba9cdf1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af41dd6cd28241f89a07d594aa2abfac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joeky00/ink-bot-react/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cd51a73"
      },
      "source": [
        "# Task\n",
        "Correct the errors in the provided notebook to ensure the AI model can be trained on the uploaded datasets (\"football_players.csv\") and respond to user questions using the trained model and external APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f2302d0"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### Subtask:\n",
        "Run the cell that loads the `football_players.csv` file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357dc7e2"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the file `football_players.csv` was not found in the `/content/` directory. However, cell `4isnmixin1oW` successfully loaded the data without specifying the full path, implying the file is in the current working directory. Cell `JFeq6oDjncxI` also confirms a csv file exists in `/content/`. The cell `40f4554c` has a FileNotFoundError. The cell `4isnmixin1oW` successfully loaded the data. The instruction is to run the cell that loads the data, which is cell `4isnmixin1oW`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "060ac35d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the CSV failed with a `FileNotFoundError`. The previous cell `JfEq6oDjncxI` listed the files in `/content`, confirming that the file `football_players.csv` is present in that directory. Therefore, I should try loading the file again, but this time specifying the full path `/content/football_players.csv`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465832fc"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "### Subtask:\n",
        "Load the `football_players.csv` file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5494d6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the dataframe failed due to a FileNotFoundError. I will try listing the files in the `/content/` directory again to confirm the file path and then attempt to load the CSV using the confirmed path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb81de0"
      },
      "source": [
        "**Reasoning**:\n",
        "The file `football_players.csv` is still not found in the `/content/` directory based on the output of `os.listdir`. However, there are several zip files listed, including `archive(1).zip` which might contain the required CSV. I will try to unzip `archive(1).zip` and then list the files again to see if the CSV file appears, and then attempt to load it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26e17f49"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully extracted `football_players.csv` from the zip file. Now I can proceed to load the CSV file into a pandas DataFrame and display the head as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b4670f9"
      },
      "source": [
        "## Generate qa pairs\n",
        "\n",
        "### Subtask:\n",
        "Run the cell that defines the `generate_qa_pairs` function and generates the `qa_data` using the loaded DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c5299e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to define the `generate_qa_pairs` function and generate `qa_data` failed because the code cell containing these steps was not executed. Executing this cell will define the function and then call it to create the `qa_data` variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6be1ee8"
      },
      "source": [
        "## Save qa data\n",
        "\n",
        "### Subtask:\n",
        "Run the cell that saves the generated QA pairs to a CSV file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4b2b2d9"
      },
      "source": [
        "**Reasoning**:\n",
        "The `qa_data` variable has been successfully generated in the previous step. Now, I need to save this data to a CSV file named \"qa_dataset.csv\". This involves importing the `csv` module if not already imported, opening the file in write mode, creating a CSV writer, writing the header row, writing the `qa_data`, and finally printing a success message. All these steps can be grouped into a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2caec6a"
      },
      "source": [
        "## Load model and tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Run the cells that load the FLAN-T5 model and tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ca8f59"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the code cell that loads the FLAN-T5 model and tokenizer and confirms successful loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592e2f72"
      },
      "source": [
        "## Preprocess the dataset\n",
        "\n",
        "### Subtask:\n",
        "Run the cells that load the QA dataset from the CSV and preprocess it for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93e870c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading the QA dataset from the CSV file and then preprocessing it using the previously defined preprocess function. This can be done in two consecutive steps by running the corresponding code cells.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "030f8b81"
      },
      "source": [
        "## Setup qa pipeline\n",
        "\n",
        "### Subtask:\n",
        "Run the cell that sets up the QA pipeline with the loaded model and tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbadc998"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to set up the QA pipeline using the previously loaded model and tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bedbf6bc"
      },
      "source": [
        "## Define api functions\n",
        "\n",
        "### Subtask:\n",
        "Run the cell that defines the functions to fetch data from the sports APIs (`get_transfer_news` and `get_next_match`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f45ca57"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because the `sports_ai_response` function was not defined in the current session. This function, along with `get_transfer_news` and `get_next_match`, are defined in cell `6xS2khQY1nI4`. Executing this cell will define the required functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69c964e6"
      },
      "source": [
        "## Test sports ai response function\n",
        "\n",
        "### Subtask:\n",
        "Run the cell that tests the `sports_ai_response` function with example questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3b22c68"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the cell that calls the `sports_ai_response` function with example questions to test its functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd9ebaea"
      },
      "source": [
        "## Launch gradio interface\n",
        "\n",
        "### Subtask:\n",
        "Run the cell that sets up and launches the Gradio interface for the chatbot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67a0bfc2"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to run the cell that sets up and launches the Gradio interface. This requires executing the provided code block that uses the `gradio` library and the previously defined `sports_ai_response` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d71c45e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "## Summary of Solving Process\n",
        "\n",
        "The goal was to correct errors in a notebook to load football player data, set up a QA system using a language model and external APIs, and create a Gradio interface to interact with the system.\n",
        "\n",
        "The solving process involved several steps:\n",
        "\n",
        "1.  **Data Loading:** Initially, the `football_players.csv` file was not found directly in the `/content/` directory. By listing the directory contents, it was discovered the file was inside `archive(1).zip`. Extracting this zip file made the CSV accessible, allowing it to be successfully loaded into a pandas DataFrame.\n",
        "2.  **QA Data Generation:** A function `generate_qa_pairs` was defined and used to create question-answer pairs based on the loaded player data. These pairs were then successfully saved to `qa_dataset.csv`.\n",
        "3.  **Model Loading and Preprocessing:** The `google/flan-t5-small` model and tokenizer were loaded using the `transformers` library. The generated `qa_dataset.csv` was loaded using `load_dataset`, and a preprocessing function was applied to tokenize the data for potential training (although the training step itself was not explicitly part of the provided process).\n",
        "4.  **API Function Definition:** Functions `get_transfer_news` and `get_next_match` were defined to fetch data from external sports APIs using predefined API keys. A `sports_ai_response` function was created to act as the main handler for user input, directing questions to either the loaded player data (via a `question-answering` pipeline using a different model, `distilbert-base-cased-distilled-squad`), or the external API functions based on keywords.\n",
        "5.  **Testing and Interface:** The `sports_ai_response` function was successfully tested with example questions. Finally, a Gradio interface was set up using the `chatbot_interface` wrapper function around `sports_ai_response` and launched to provide a user-friendly way to interact with the system.\n",
        "\n",
        "## Data Analysis Key Findings\n",
        "\n",
        "*   The primary data loading issue was resolved by extracting the `football_players.csv` file from a zip archive (`archive(1).zip`).\n",
        "*   Question-answer pairs were successfully generated from the loaded football player data and saved to a CSV file (`qa_dataset.csv`).\n",
        "*   The `google/flan-t5-small` model and tokenizer were loaded, and the generated QA dataset was preprocessed (tokenized).\n",
        "*   Functions to interact with external sports APIs for transfer news and upcoming matches were successfully defined.\n",
        "*   A `sports_ai_response` function was created to route user queries to either the loaded player data or the API functions. This function utilized a separate `question-answering` pipeline (`distilbert-base-cased-distilled-squad`) for player-specific questions, distinct from the previously loaded FLAN-T5 model used in the text-to-text generation pipeline.\n",
        "*   A Gradio interface was successfully launched, allowing interaction with the `sports_ai_response` function.\n",
        "\n",
        "## Insights or Next Steps\n",
        "\n",
        "*   Clarify the intended use of the two different models loaded (`google/flan-t5-small` and `distilbert-base-cased-distilled-squad`) within the `sports_ai_response` function. The FLAN-T5 model was loaded for a `text2text-generation` pipeline, while the `distilbert` model was loaded for the `question-answering` pipeline actually used for player queries.\n",
        "*   Consider training the `google/flan-t5-small` model on the generated `qa_dataset` using the preprocessed `tokenized_dataset` to potentially improve its performance on player-specific questions based on the provided data, and then integrate the trained model into the `sports_ai_response` function for player queries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d7be66"
      },
      "source": [
        "# Task\n",
        "Use the files \"archive(1).zip\", \"archive(2).zip\", \"archive(3).zip\", \"archive(5).zip\", and \"archive(6).zip\" as datasets to generate more question-answer pairs for training and as additional context for the question-answering part of the AI, and ensure the AI can use all the provided data to answer questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13b65057"
      },
      "source": [
        "## Enhance qa pair generation\n",
        "\n",
        "### Subtask:\n",
        "Modify the `generate_qa_pairs` function or create new functions to generate QA pairs from the newly loaded dataframes (`df_top250`, `df_fifa`, `df_data`, `df_dataset`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dab0736d"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define new functions to generate QA pairs from each of the newly loaded dataframes, considering their specific column names and structures. Then, I will call these functions to generate QA pairs and store them in separate lists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917ac442"
      },
      "source": [
        "**Reasoning**:\n",
        "I have generated QA pairs from the new dataframes. Now I need to combine the QA pairs from all dataframes (including the original `qa_data`) into a single list to create a comprehensive dataset for potential training and use as context. Then I will save this combined dataset to a CSV file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9b2439f"
      },
      "source": [
        "**Reasoning**:\n",
        "I have generated and saved the combined QA dataset. The next step is to load this combined dataset using the `datasets` library and then preprocess it using the previously defined `preprocess` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc67f9de"
      },
      "source": [
        "## Combine and save qa data\n",
        "\n",
        "### Subtask:\n",
        "Combine the newly generated QA pairs with the existing `qa_data` and save the combined data to `qa_dataset.csv`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7daba118"
      },
      "source": [
        "## Update preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Re-run the preprocessing steps to include the new QA data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0accac26"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the combined QA dataset from the CSV file and apply the preprocessing function to it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0eb94a8"
      },
      "source": [
        "# Cleaned-up Sports AI Chatbot Notebook\n",
        "\n",
        "This notebook contains the essential code to run the Sports AI Chatbot, including data loading, QA generation, model setup, API integration, and the Gradio interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed485e71"
      },
      "source": [
        "## 1. Extract Data from Zip Files\n",
        "\n",
        "Extract the necessary CSV files from the uploaded zip archives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8c43883",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362a812f-fcd6-401f-92ce-90dacacc2a6b"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import glob # Import glob to find files\n",
        "\n",
        "# Use glob to find zip files in the /content directory\n",
        "zip_files = glob.glob('/content/archive(*).zip')\n",
        "\n",
        "extract_dir = '/content/'\n",
        "\n",
        "for zip_file_path in zip_files:\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(f\"Successfully extracted {zip_file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {zip_file_path} not found.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: {zip_file_path} is a bad zip file.\")\n",
        "\n",
        "# List files in /content after extraction to confirm\n",
        "print(\"\\nFiles in /content after extraction:\")\n",
        "for file in os.listdir(extract_dir):\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files in /content after extraction:\n",
            ".config\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80dbe7c4"
      },
      "source": [
        "## 2. Load DataFrames\n",
        "\n",
        "Load the extracted CSV files into pandas DataFrames. These will be used for generating QA pairs and as context for the AI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecc7275d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555cf264-dd54-496f-8e71-07577c4fa37c",
        "collapsed": true
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load football_players.csv (original dataset)\n",
        "try:\n",
        "    df = pd.read_csv('/content/football_players.csv')\n",
        "    print(\"Loaded football_players.csv\")\n",
        "    # display(df.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: football_players.csv not found.\")\n",
        "    df = None # Ensure df is None if file not found\n",
        "\n",
        "# Load and display top250-00-19.csv\n",
        "try:\n",
        "    df_top250 = pd.read_csv('/content/top250-00-19.csv')\n",
        "    print(\"Loaded top250-00-19.csv\")\n",
        "    # display(df_top250.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: top250-00-19.csv not found.\")\n",
        "    df_top250 = None # Ensure df_top250 is None if file not found\n",
        "\n",
        "# Load and display fifa_players.csv\n",
        "try:\n",
        "    df_fifa = pd.read_csv('/content/fifa_players.csv')\n",
        "    print(\"Loaded fifa_players.csv\")\n",
        "    # display(df_fifa.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: fifa_players.csv not found.\")\n",
        "    df_fifa = None # Ensure df_fifa is None if file not found\n",
        "\n",
        "# Load and display data.csv\n",
        "try:\n",
        "    df_data = pd.read_csv('/content/data.csv')\n",
        "    print(\"Loaded data.csv\")\n",
        "    # display(df_data.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: data.csv not found.\")\n",
        "    df_data = None # Ensure df_data is None if file not found\n",
        "\n",
        "# Load and display dataset.csv\n",
        "try:\n",
        "    df_dataset = pd.read_csv('/content/dataset.csv')\n",
        "    print(\"Loaded dataset.csv\")\n",
        "    # display(df_dataset.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: dataset.csv not found.\")\n",
        "    df_dataset = None # Ensure df_dataset is None if file not found"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded football_players.csv\n",
            "Loaded top250-00-19.csv\n",
            "Loaded fifa_players.csv\n",
            "Loaded data.csv\n",
            "Loaded dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f8c0a40"
      },
      "source": [
        "## 3. Generate and Combine QA Pairs\n",
        "\n",
        "Define functions to generate QA pairs from each DataFrame and combine them into a single dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a52ea79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0595816-14a1-4d6e-c80e-b1b80da2b699"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Original QA generation function (assuming df is loaded)\n",
        "def generate_qa_pairs(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"Original df is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"Player\", \"Unknown\")\n",
        "        nationality = player.get(\"Origin\", \"Unknown\")\n",
        "        club = player.get(\"To(Club)\", \"Unknown\")\n",
        "        position = player.get(\"Position\", \"Unknown\")\n",
        "        fee = player.get(\"Fee(€ mln)\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club does {name} play for?\",\n",
        "            f\"What is the transfer fee for {name} in € mln?\",\n",
        "            f\"What position does {name} play?\",\n",
        "            f\"What is the nationality of {name}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{name} plays for {club}.\",\n",
        "            f\"The transfer fee for {name} was {fee} € mln.\",\n",
        "            f\"{name} plays as a {position}.\",\n",
        "            f\"{name} is from {nationality}.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_top250\n",
        "def generate_qa_pairs_top250(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_top250 is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"Name\", \"Unknown\")\n",
        "        position = player.get(\"Position\", \"Unknown\")\n",
        "        team_from = player.get(\"Team_from\", \"Unknown\")\n",
        "        team_to = player.get(\"Team_to\", \"Unknown\")\n",
        "        transfer_fee = player.get(\"Transfer_fee\", \"Unknown\")\n",
        "        season = player.get(\"Season\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club did {name} transfer to in the {season} season?\",\n",
        "            f\"What was the transfer fee for {name} in the {season} season?\",\n",
        "            f\"What position did {name} play?\",\n",
        "            f\"Which club did {name} transfer from in the {season} season?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{name} transferred to {team_to} in the {season} season.\",\n",
        "            f\"The transfer fee for {name} in the {season} season was {transfer_fee}.\",\n",
        "            f\"{name} played as a {position}.\",\n",
        "            f\"{name} transferred from {team_from} in the {season} season.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_fifa\n",
        "def generate_qa_pairs_fifa(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_fifa is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"name\", \"Unknown\")\n",
        "        full_name = player.get(\"full_name\", \"Unknown\")\n",
        "        nationality = player.get(\"nationality\", \"Unknown\")\n",
        "        overall_rating = player.get(\"overall_rating\", \"Unknown\")\n",
        "        age = player.get(\"age\", \"Unknown\")\n",
        "        positions = player.get(\"positions\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"What is the full name of {name}?\",\n",
        "            f\"What is the overall rating of {name}?\",\n",
        "            f\"What is the nationality of {name}?\",\n",
        "            f\"What is the age of {name}?\",\n",
        "            f\"What positions does {name} play?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"The full name of {name} is {full_name}.\",\n",
        "            f\"The overall rating of {name} is {overall_rating}.\",\n",
        "            f\"{name} is from {nationality}.\",\n",
        "            f\"{name} is {age} years old.\",\n",
        "            f\"{name} plays in the {positions} positions.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_data\n",
        "def generate_qa_pairs_data(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_data is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        match_event = df.sample(1).iloc[0]\n",
        "        player = match_event.get(\"Player\", \"Unknown\")\n",
        "        season = match_event.get(\"Season\", \"Unknown\")\n",
        "        competition = match_event.get(\"Competition\", \"Unknown\")\n",
        "        club = match_event.get(\"Club\", \"Unknown\")\n",
        "        opponent = match_event.get(\"Opponent\", \"Unknown\")\n",
        "        matchday = match_event.get(\"Matchday\", \"Unknown\")\n",
        "        date = match_event.get(\"Date\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"What competition did {player} play in during the {season} season on matchday {matchday}?\",\n",
        "            f\"Which club did {player} play for in the match against {opponent} on {date}?\",\n",
        "            f\"What was the opponent when {player} played for {club} on {date}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{player} played in the {competition} competition during the {season} season on matchday {matchday}.\",\n",
        "            f\"{player} played for {club} in the match against {opponent} on {date}.\",\n",
        "            f\"The opponent was {opponent} when {player} played for {club} on {date}.\",\n",
        "        ]\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_dataset\n",
        "def generate_qa_pairs_dataset(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_dataset is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player_transfer = df.sample(1).iloc[0]\n",
        "        player = player_transfer.get(\"Player\", \"Unknown\")\n",
        "        year = player_transfer.get(\"Year\", \"Unknown\")\n",
        "        fee = player_transfer.get(\"Fee\", \"Unknown\")\n",
        "        from_club = player_transfer.get(\"From Club\", \"Unknown\")\n",
        "        to_club = player_transfer.get(\"To Club\", \"Unknown\")\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club did {player} transfer to in {year}?\",\n",
        "            f\"What was the transfer fee for {player} in {year}?\",\n",
        "            f\"Which club did {player} transfer from in {year}?\",\n",
        "            f\"In what year did {player} transfer from {from_club} to {to_club}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{player} transferred to {to_club} in {year}.\",\n",
        "            f\"The transfer fee for {player} in {year} was {fee}.\",\n",
        "            f\"{player} transferred from {from_club} in {year}.\",\n",
        "            f\"{player} transferred from {from_club} to {to_club} in {year}.\",\n",
        "        ]\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "\n",
        "# Generate QA pairs from each dataframe that was successfully loaded\n",
        "all_qa_data = []\n",
        "if df is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs(df, num_samples=50)) # Increased samples for more data\n",
        "if df_top250 is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_top250(df_top250, num_samples=50)) # Increased samples\n",
        "if df_fifa is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_fifa(df_fifa, num_samples=50)) # Increased samples\n",
        "if df_data is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_data(df_data, num_samples=50)) # Increased samples\n",
        "if df_dataset is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_dataset(df_dataset, num_samples=50)) # Increased samples\n",
        "\n",
        "\n",
        "# Save the combined QA pairs into a CSV file\n",
        "with open(\"combined_qa_dataset.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"question\", \"answer\"])\n",
        "    writer.writerows(all_qa_data)\n",
        "\n",
        "print(f\"✅ Saved combined QA dataset with {len(all_qa_data)} pairs as combined_qa_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved combined QA dataset with 1000 pairs as combined_qa_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eadd7cc"
      },
      "source": [
        "## 4. Load Model and Tokenizer\n",
        "\n",
        "Load the FLAN-T5 model and tokenizer for text generation (and potentially for fine-tuning if needed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d41a97e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff2acdc-f8d0-4c79-974f-45928e9b3dbd"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "print(\"✅ Loaded flan-t5-small model successfully!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded flan-t5-small model successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06f42041"
      },
      "source": [
        "## 5. Preprocess the Combined Dataset\n",
        "\n",
        "Load the combined QA dataset and preprocess it for potential use with a language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99ba7749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198,
          "referenced_widgets": [
            "536911b18093489bb600c4f9055d1461",
            "e68f8fb9a32647b2826f5682974d94ce",
            "a1e2dacf5cc54ba09fd401c9e32eaf7a",
            "a5db64b8992f4f52885c2efb137431ab",
            "16db819607214e0c9602f28f0f04f6e2",
            "cfb4810abefe40aea7869cdb7c45a0b0",
            "5cc77cf955da4a8f8f821dba62c52290",
            "304fa4ccdc464a9096199810d427eb78",
            "0959b46a68b64637a0f092dcca613f0c",
            "2efdd00a12994706a90bf4e3b46b5b0f",
            "9b90a42fd1984b03a86ab57a041937b3",
            "e4f2f0fda66143e08d9df731792737de",
            "9fb72e842ec24ddb834cee250a6e3a52",
            "ca6cb43f8582489c9d54f18b1baa2342",
            "eefdfc24b8914da499bdb29fa84c981b",
            "b556264ba9254ab1a4a2f7a9cb554fb3",
            "73a25d9061fe47c4aaed762921c1a3a1",
            "813c87e2ad8b44868ae97aeae0a75f1a",
            "eb00f5aec24e4532abcbab621f8994c1",
            "87bb6c820e3a4fe3827b4f50ec914efe",
            "99cedcbfd2c9444a83d72492ba9cdf1a",
            "af41dd6cd28241f89a07d594aa2abfac"
          ]
        },
        "outputId": "d1486027-6062-4fb6-9676-88fb84e7fd5e"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the combined dataset from the CSV file\n",
        "combined_dataset = load_dataset('csv', data_files='combined_qa_dataset.csv')\n",
        "\n",
        "# Load tokenizer (ensure it's loaded if session restarted)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\") # Assuming tokenizer is already loaded\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(example):\n",
        "    input_text = \"question: \" + example[\"question\"]\n",
        "    target_text = example[\"answer\"]\n",
        "    tokenized_input = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=128)\n",
        "    tokenized_target = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=32)\n",
        "\n",
        "    tokenized_input[\"labels\"] = tokenized_target[\"input_ids\"]\n",
        "    return tokenized_input\n",
        "\n",
        "# Apply preprocessing to the combined dataset\n",
        "tokenized_combined_dataset = combined_dataset.map(preprocess)\n",
        "\n",
        "# Preview tokenized combined data\n",
        "print(\"Preview of tokenized combined dataset:\")\n",
        "print(tokenized_combined_dataset[\"train\"][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "536911b18093489bb600c4f9055d1461"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4f2f0fda66143e08d9df731792737de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preview of tokenized combined dataset:\n",
            "{'question': 'Which club does Luka Jović play for?', 'answer': 'Luka Jović plays for Real Madrid.', 'input_ids': [822, 10, 4073, 1886, 405, 2318, 1258, 446, 9881, 2, 577, 21, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2318, 1258, 446, 9881, 2, 4805, 21, 2977, 12033, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8f4f8e2"
      },
      "source": [
        "## 6. Define API and AI Response Functions\n",
        "\n",
        "Define the functions to interact with external APIs for news and matches, and the main AI response function that routes user queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5fe0cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30103d8-538e-4a07-9e78-822bc2c8b054"
      },
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd # Import pandas for DataFrame handling\n",
        "\n",
        "# API keys (Replace with secure handling in deployment)\n",
        "# It's recommended to use environment variables or Colab secrets for API keys\n",
        "news_api_key = \"b311a02382fa4a88b9d1b4bfc74bb051\"\n",
        "football_api_key = \"5e8310b5845626994bcbf672a6ff5b60\"\n",
        "\n",
        "# Load the question-answering pipeline model (used for player queries)\n",
        "try:\n",
        "    # Note: This uses a different model (distilbert-base-cased-distilled-squad)\n",
        "    # than the FLAN-T5 model loaded earlier.\n",
        "    qa_pipeline_player = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    print(\"✅ Loaded question-answering pipeline for player queries successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not load question-answering pipeline: {e}\")\n",
        "    qa_pipeline_player = None\n",
        "\n",
        "\n",
        "def get_transfer_news():\n",
        "    url = f\"https://newsapi.org/v2/everything?q=football transfers&language=en&sortBy=publishedAt&apiKey={news_api_key}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"articles\"):\n",
        "            article = data[\"articles\"][0]\n",
        "            return f\"📰 Latest Transfer: \\\"{article['title']}\\\" (Source: {article['source']['name']})\"\n",
        "        return \"⚠️ No transfer news available.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching transfer news: {e}\"\n",
        "\n",
        "\n",
        "def get_next_match():\n",
        "    headers = {\n",
        "        \"x-rapidapi-host\": \"v3.football.api-sports.io\",\n",
        "        \"x-apisports-key\": football_api_key,\n",
        "    }\n",
        "    # Changed season to 2023 as per API error suggestion for free plan\n",
        "    url = \"https://v3.football.api-sports.io/fixtures?league=39&season=2023&next=1\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"response\"):\n",
        "            match = data[\"response\"][0][\"teams\"]\n",
        "            return f\"⚽ Next Match: {match['home']['name']} vs {match['away']['name']}\"\n",
        "        return \"⚠️ No upcoming matches.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching next match data: {e}\"\n",
        "\n",
        "\n",
        "# AI Response function\n",
        "def sports_ai_response(user_input):\n",
        "    user_input = user_input.lower()\n",
        "\n",
        "    # Combine all loaded dataframes into a single context string for the QA pipeline\n",
        "    combined_context = \"\"\n",
        "    if 'df' in globals() and isinstance(df, pd.DataFrame):\n",
        "        combined_context += df.to_string() + \"\\n\\n\"\n",
        "    if 'df_top250' in globals() and isinstance(df_top250, pd.DataFrame):\n",
        "        combined_context += df_top250.to_string() + \"\\n\\n\"\n",
        "    if 'df_fifa' in globals() and isinstance(df_fifa, pd.DataFrame):\n",
        "        combined_context += df_fifa.to_string() + \"\\n\\n\"\n",
        "    if 'df_data' in globals() and isinstance(df_data, pd.DataFrame):\n",
        "        combined_context += df_data.to_string() + \"\\n\\n\"\n",
        "    if 'df_dataset' in globals() and isinstance(df_dataset, pd.DataFrame):\n",
        "        combined_context += df_dataset.to_string() + \"\\n\\n\"\n",
        "\n",
        "\n",
        "    # Use the question-answering pipeline for factual player-based questions\n",
        "    # Check for keywords that indicate a player-specific question\n",
        "    player_keywords = [\"who\", \"what\", \"how\", \"when\", \"is\", \"was\", \"paid\", \"salary\", \"fee\", \"rating\", \"position\", \"nationality\", \"age\", \"height\", \"weight\", \"club\", \"team\", \"transfer\", \"season\", \"competition\", \"matchday\", \"opponent\", \"date\", \"plays for\"]\n",
        "    if any(k in user_input for k in player_keywords):\n",
        "        if qa_pipeline_player and combined_context:\n",
        "             try:\n",
        "                 # Use the 'qa_pipeline_player' pipeline with combined context\n",
        "                 result = qa_pipeline_player(question=user_input, context=combined_context)\n",
        "                 # Check if the answer is likely valid\n",
        "                 if result and result.get('answer') and len(result['answer'].split()) > 1 and result.get('score', 0) > 0.1: # Add a score threshold\n",
        "                     return f\"🤖 Answer: {result['answer']}\"\n",
        "                 else:\n",
        "                     # Fallback if QA doesn't find a good answer in the combined context\n",
        "                     print(\"QA pipeline did not find a confident answer in combined context. Checking APIs...\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Error during QA pipeline processing: {e}\")\n",
        "                 return \"⚠️ An error occurred while trying to answer your question.\"\n",
        "        else:\n",
        "            print(\"QA pipeline not loaded or no dataframes loaded for context. Checking APIs...\")\n",
        "\n",
        "\n",
        "    # Use API if question is about transfers (and QA didn't find a good answer)\n",
        "    if \"transfer\" in user_input or \"signed\" in user_input or \"latest news\" in user_input:\n",
        "        return get_transfer_news()\n",
        "\n",
        "    # Use API if question is about next match (and QA didn't find a good answer)\n",
        "    elif any(kw in user_input for kw in [\"next match\", \"upcoming match\", \"who is playing\", \"next premier league game\"]):\n",
        "        return get_next_match()\n",
        "\n",
        "\n",
        "    # Catch-all fallback if no specific intent matched or QA/API failed\n",
        "    return \"⚽ I'm still learning. Try asking about a player, transfer, or upcoming match!\"\n",
        "\n",
        "print(\"✅ Defined sports_ai_response and API helper functions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Could not load question-answering pipeline: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\n",
            "cannot import name 'convert_batch_encoding' from 'transformers.tf_utils' (/usr/local/lib/python3.11/dist-packages/transformers/tf_utils.py)\n",
            "✅ Defined sports_ai_response and API helper functions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97af9394"
      },
      "source": [
        "## 7. Launch Gradio Interface\n",
        "\n",
        "Set up and launch the Gradio interface to interact with the chatbot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68fecbad"
      },
      "source": [
        "!pip install gradio --quiet\n",
        "import gradio as gr\n",
        "\n",
        "def chatbot_interface(message):\n",
        "    # Ensure dataframes are loaded if they are used as context in sports_ai_response\n",
        "    # This might be redundant if the loading cells are guaranteed to run before this,\n",
        "    # but adds robustness if the execution order is not strictly controlled.\n",
        "    global df, df_top250, df_fifa, df_data, df_dataset\n",
        "    # You might need to re-run the data loading cell or ensure persistence\n",
        "\n",
        "    return sports_ai_response(message)\n",
        "\n",
        "demo = gr.Interface(fn=chatbot_interface,\n",
        "                    inputs=\"text\",\n",
        "                    outputs=\"text\",\n",
        "                    title=\"⚽ Sports AI Chatbot\",\n",
        "                    description=\"Ask about football players, transfers, or next matches!\")\n",
        "\n",
        "# To run in Colab, use share=True to get a public URL\n",
        "# For deployment on platforms like Render, share=False is typical,\n",
        "# and the platform handles exposing the service.\n",
        "demo.launch(share=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4f8fc89",
        "outputId": "5331b062-193a-439d-9551-0dabf1464e27"
      },
      "source": [
        "# Install a compatible version of transformers\n",
        "!pip install transformers==4.30.2 --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a1e0908c",
        "outputId": "35b4d101-b750-4ea7-bca5-63f6639e26d3"
      },
      "source": [
        "# Use a different approach to ensure compatible transformers installation\n",
        "!pip uninstall transformers -y -q\n",
        "!pip install transformers --no-cache-dir --upgrade -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "print(\"✅ Loaded flan-t5-small model successfully!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m184.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m307.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'GenerationMixin' from 'transformers.generation' (/usr/local/lib/python3.11/dist-packages/transformers/generation/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3762839760.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"google/flan-t5-small\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Loaded flan-t5-small model successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;34m\"token\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         ]\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0mhub_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhub_kwargs_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseAutoModelClass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0;31m# Download model and configuration from huggingface.co and cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseAutoModelClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpoint_placeholder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    655\u001b[0m         )\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    669\u001b[0m         return docstring.replace(\n\u001b[1;32m    670\u001b[0m             \u001b[0;34m\"one of the model classes of the library \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;34mf\"one of the model classes of the library (with a {head_doc} head) \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         )\n\u001b[1;32m    673\u001b[0m     return docstring.replace(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mThe\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mto\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m                 \u001b[0mThe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mto\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2276\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2277\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2304\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2306\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2304\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mACT2FN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mcache_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDynamicCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoderDecoderCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_attn_mask_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttentionMaskConverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientCheckpointingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'GenerationMixin' from 'transformers.generation' (/usr/local/lib/python3.11/dist-packages/transformers/generation/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fb8f7eb",
        "outputId": "d0063f3a-e1fb-45e5-9cbd-d72d1ddb0590"
      },
      "source": [
        "# Install a potentially more stable version of transformers\n",
        "!pip install transformers==4.28.0 --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0187bd8",
        "outputId": "ee1756b6-2d71-44bd-dc20-455d0e8951de"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import glob # Import glob to find files\n",
        "\n",
        "# Use glob to find zip files in the /content directory - using a simpler pattern\n",
        "zip_files = glob.glob('/content/*.zip')\n",
        "\n",
        "extract_dir = '/content/'\n",
        "\n",
        "for zip_file_path in zip_files:\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(f\"Successfully extracted {zip_file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {zip_file_path} not found.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: {zip_file_path} is a bad zip file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing {zip_file_path}: {e}\")\n",
        "\n",
        "\n",
        "# List files in /content after extraction to confirm\n",
        "print(\"\\nFiles in /content after extraction:\")\n",
        "try:\n",
        "    for file in os.listdir(extract_dir):\n",
        "        print(file)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Directory {extract_dir} not found.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted /content/archive(5).zip\n",
            "Successfully extracted /content/archive(2).zip\n",
            "Successfully extracted /content/archive(6).zip\n",
            "Successfully extracted /content/archive(3).zip\n",
            "Successfully extracted /content/archive(1).zip\n",
            "\n",
            "Files in /content after extraction:\n",
            ".config\n",
            "archive(5).zip\n",
            "data.csv\n",
            "archive(2).zip\n",
            "top250-00-19.csv\n",
            "archive(6).zip\n",
            "archive(3).zip\n",
            "dataset.csv\n",
            "fifa_players.csv\n",
            "archive(1).zip\n",
            "football_players.csv\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb974327",
        "outputId": "e75d26be-b1c4-4f2f-dd6e-058c2c307ec1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load football_players.csv (original dataset)\n",
        "try:\n",
        "    df = pd.read_csv('/content/football_players.csv')\n",
        "    print(\"Loaded football_players.csv\")\n",
        "    # display(df.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: football_players.csv not found.\")\n",
        "    df = None # Ensure df is None if file not found\n",
        "\n",
        "# Load and display top250-00-19.csv\n",
        "try:\n",
        "    df_top250 = pd.read_csv('/content/top250-00-19.csv')\n",
        "    print(\"Loaded top250-00-19.csv\")\n",
        "    # display(df_top250.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: top250-00-19.csv not found.\")\n",
        "    df_top250 = None # Ensure df_top250 is None if file not found\n",
        "\n",
        "# Load and display fifa_players.csv\n",
        "try:\n",
        "    df_fifa = pd.read_csv('/content/fifa_players.csv')\n",
        "    print(\"Loaded fifa_players.csv\")\n",
        "    # display(df_fifa.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: fifa_players.csv not found.\")\n",
        "    df_fifa = None # Ensure df_fifa is None if file not found\n",
        "\n",
        "# Load and display data.csv\n",
        "try:\n",
        "    df_data = pd.read_csv('/content/data.csv')\n",
        "    print(\"Loaded data.csv\")\n",
        "    # display(df_data.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: data.csv not found.\")\n",
        "    df_data = None # Ensure df_data is None if file not found\n",
        "\n",
        "# Load and display dataset.csv\n",
        "try:\n",
        "    df_dataset = pd.read_csv('/content/dataset.csv')\n",
        "    print(\"Loaded dataset.csv\")\n",
        "    # display(df_dataset.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: dataset.csv not found.\")\n",
        "    df_dataset = None # Ensure df_dataset is None if file not found"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded football_players.csv\n",
            "Loaded top250-00-19.csv\n",
            "Loaded fifa_players.csv\n",
            "Loaded data.csv\n",
            "Loaded dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb4b37d2",
        "outputId": "25b64434-7ca4-47f2-cc21-06002f37e256"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Original QA generation function (assuming df is loaded)\n",
        "def generate_qa_pairs(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"Original df is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"Player\", \"Unknown\")\n",
        "        nationality = player.get(\"Origin\", \"Unknown\")\n",
        "        club = player.get(\"To(Club)\", \"Unknown\")\n",
        "        position = player.get(\"Position\", \"Unknown\")\n",
        "        fee = player.get(\"Fee(€ mln)\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club does {name} play for?\",\n",
        "            f\"What is the transfer fee for {name} in € mln?\",\n",
        "            f\"What position does {name} play?\",\n",
        "            f\"What is the nationality of {name}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{name} plays for {club}.\",\n",
        "            f\"The transfer fee for {name} was {fee} € mln.\",\n",
        "            f\"{name} plays as a {position}.\",\n",
        "            f\"{name} is from {nationality}.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_top250\n",
        "def generate_qa_pairs_top250(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_top250 is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"Name\", \"Unknown\")\n",
        "        position = player.get(\"Position\", \"Unknown\")\n",
        "        team_from = player.get(\"Team_from\", \"Unknown\")\n",
        "        team_to = player.get(\"Team_to\", \"Unknown\")\n",
        "        transfer_fee = player.get(\"Transfer_fee\", \"Unknown\")\n",
        "        season = player.get(\"Season\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club did {name} transfer to in the {season} season?\",\n",
        "            f\"What was the transfer fee for {name} in the {season} season?\",\n",
        "            f\"What position did {name} play?\",\n",
        "            f\"Which club did {name} transfer from in the {season} season?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{name} transferred to {team_to} in the {season} season.\",\n",
        "            f\"The transfer fee for {name} in the {season} season was {transfer_fee}.\",\n",
        "            f\"{name} played as a {position}.\",\n",
        "            f\"{name} transferred from {team_from} in the {season} season.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_fifa\n",
        "def generate_qa_pairs_fifa(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_fifa is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"name\", \"Unknown\")\n",
        "        full_name = player.get(\"full_name\", \"Unknown\")\n",
        "        nationality = player.get(\"nationality\", \"Unknown\")\n",
        "        overall_rating = player.get(\"overall_rating\", \"Unknown\")\n",
        "        age = player.get(\"age\", \"Unknown\")\n",
        "        positions = player.get(\"positions\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"What is the full name of {name}?\",\n",
        "            f\"What is the overall rating of {name}?\",\n",
        "            f\"What is the nationality of {name}?\",\n",
        "            f\"What is the age of {name}?\",\n",
        "            f\"What positions does {name} play?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"The full name of {name} is {full_name}.\",\n",
        "            f\"The overall rating of {name} is {overall_rating}.\",\n",
        "            f\"{name} is from {nationality}.\",\n",
        "            f\"{name} is {age} years old.\",\n",
        "            f\"{name} plays in the {positions} positions.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_data\n",
        "def generate_qa_pairs_data(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_data is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        match_event = df.sample(1).iloc[0]\n",
        "        player = match_event.get(\"Player\", \"Unknown\")\n",
        "        season = match_event.get(\"Season\", \"Unknown\")\n",
        "        competition = match_event.get(\"Competition\", \"Unknown\")\n",
        "        club = match_event.get(\"Club\", \"Unknown\")\n",
        "        opponent = match_event.get(\"Opponent\", \"Unknown\")\n",
        "        matchday = match_event.get(\"Matchday\", \"Unknown\")\n",
        "        date = match_event.get(\"Date\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"What competition did {player} play in during the {season} season on matchday {matchday}?\",\n",
        "            f\"Which club did {player} play for in the match against {opponent} on {date}?\",\n",
        "            f\"What was the opponent when {player} played for {club} on {date}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{player} played in the {competition} competition during the {season} season on matchday {matchday}.\",\n",
        "            f\"{player} played for {club} in the match against {opponent} on {date}.\",\n",
        "            f\"The opponent was {opponent} when {player} played for {club} on {date}.\",\n",
        "        ]\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_dataset\n",
        "def generate_qa_pairs_dataset(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_dataset is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player_transfer = df.sample(1).iloc[0]\n",
        "        player = player_transfer.get(\"Player\", \"Unknown\")\n",
        "        year = player_transfer.get(\"Year\", \"Unknown\")\n",
        "        fee = player_transfer.get(\"Fee\", \"Unknown\")\n",
        "        from_club = player_transfer.get(\"From Club\", \"Unknown\")\n",
        "        to_club = player_transfer.get(\"To Club\", \"Unknown\")\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club did {player} transfer to in {year}?\",\n",
        "            f\"What was the transfer fee for {player} in {year}?\",\n",
        "            f\"Which club did {player} transfer from in {year}?\",\n",
        "            f\"In what year did {player} transfer from {from_club} to {to_club}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{player} transferred to {to_club} in {year}.\",\n",
        "            f\"The transfer fee for {player} in {year} was {fee}.\",\n",
        "            f\"{player} transferred from {from_club} in {year}.\",\n",
        "            f\"{player} transferred from {from_club} to {to_club} in {year}.\",\n",
        "        ]\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "\n",
        "# Generate QA pairs from each dataframe that was successfully loaded\n",
        "all_qa_data = []\n",
        "if df is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs(df, num_samples=50)) # Increased samples for more data\n",
        "if df_top250 is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_top250(df_top250, num_samples=50)) # Increased samples\n",
        "if df_fifa is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_fifa(df_fifa, num_samples=50)) # Increased samples\n",
        "if df_data is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_data(df_data, num_samples=50)) # Increased samples\n",
        "if df_dataset is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_dataset(df_dataset, num_samples=50)) # Increased samples\n",
        "\n",
        "\n",
        "# Save the combined QA pairs into a CSV file\n",
        "with open(\"combined_qa_dataset.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"question\", \"answer\"])\n",
        "    writer.writerows(all_qa_data)\n",
        "\n",
        "print(f\"✅ Saved combined QA dataset with {len(all_qa_data)} pairs as combined_qa_dataset.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved combined QA dataset with 1000 pairs as combined_qa_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6b2a2e"
      },
      "source": [
        "# Install a compatible version of tensorflow and keras\n",
        "!pip install tensorflow==2.13.0 keras==2.13.1 --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d994b52f",
        "outputId": "2fb9d913-5cda-4423-9aa6-a4bcf5f1e73a"
      },
      "source": [
        "# Uninstall existing transformers and tensorflow installations\n",
        "!pip uninstall transformers -y -q\n",
        "!pip uninstall tensorflow -y -q\n",
        "\n",
        "# Reinstall transformers and tensorflow to resolve dependencies\n",
        "!pip install transformers tensorflow --no-cache-dir --upgrade -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m202.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m232.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m289.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m233.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m320.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m185.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbebb721",
        "outputId": "cb546a8f-bda3-49fc-8e9f-68cda8ac2c94"
      },
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd # Import pandas for DataFrame handling\n",
        "\n",
        "# API keys (Replace with secure handling in deployment)\n",
        "# It's recommended to use environment variables or Colab secrets for API keys\n",
        "news_api_key = \"b311a02382fa4a88b9d1b4bfc74bb051\"\n",
        "football_api_key = \"5e8310b5845626994bcbf672a6ff5b60\"\n",
        "\n",
        "# Load the question-answering pipeline model (used for player queries)\n",
        "try:\n",
        "    # Note: This uses a different model (distilbert-base-cased-distilled-squad)\n",
        "    # than the FLAN-T5 model loaded earlier.\n",
        "    qa_pipeline_player = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    print(\"✅ Loaded question-answering pipeline for player queries successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not load question-answering pipeline: {e}\")\n",
        "    qa_pipeline_player = None\n",
        "\n",
        "\n",
        "def get_transfer_news():\n",
        "    url = f\"https://newsapi.org/v2/everything?q=football transfers&language=en&sortBy=publishedAt&apiKey={news_api_key}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"articles\"):\n",
        "            article = data[\"articles\"][0]\n",
        "            return f\"📰 Latest Transfer: \\\"{article['title']}\\\" (Source: {article['source']['name']})\"\n",
        "        return \"⚠️ No transfer news available.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching transfer news: {e}\"\n",
        "\n",
        "\n",
        "def get_next_match():\n",
        "    headers = {\n",
        "        \"x-rapidapi-host\": \"v3.football.api-sports.io\",\n",
        "        \"x-apisports-key\": football_api_key,\n",
        "    }\n",
        "    # Changed season to 2023 as per API error suggestion for free plan\n",
        "    url = \"https://v3.football.api-sports.io/fixtures?league=39&season=2023&next=1\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"response\"):\n",
        "            match = data[\"response\"][0][\"teams\"]\n",
        "            return f\"⚽ Next Match: {match['home']['name']} vs {match['away']['name']}\"\n",
        "        return \"⚠️ No upcoming matches.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching next match data: {e}\"\n",
        "\n",
        "\n",
        "# AI Response function\n",
        "def sports_ai_response(user_input):\n",
        "    user_input = user_input.lower()\n",
        "\n",
        "    # Combine all loaded dataframes into a single context string for the QA pipeline\n",
        "    combined_context = \"\"\n",
        "    if 'df' in globals() and isinstance(df, pd.DataFrame):\n",
        "        combined_context += df.to_string() + \"\\n\\n\"\n",
        "    if 'df_top250' in globals() and isinstance(df_top250, pd.DataFrame):\n",
        "        combined_context += df_top250.to_string() + \"\\n\\n\"\n",
        "    if 'df_fifa' in globals() and isinstance(df_fifa, pd.DataFrame):\n",
        "        combined_context += df_fifa.to_string() + \"\\n\\n\"\n",
        "    if 'df_data' in globals() and isinstance(df_data, pd.DataFrame):\n",
        "        combined_context += df_data.to_string() + \"\\n\\n\"\n",
        "    if 'df_dataset' in globals() and isinstance(df_dataset, pd.DataFrame):\n",
        "        combined_context += df_dataset.to_string() + \"\\n\\n\"\n",
        "\n",
        "\n",
        "    # Use the question-answering pipeline for factual player-based questions\n",
        "    # Check for keywords that indicate a player-specific question\n",
        "    player_keywords = [\"who\", \"what\", \"how\", \"when\", \"is\", \"was\", \"paid\", \"salary\", \"fee\", \"rating\", \"position\", \"nationality\", \"age\", \"height\", \"weight\", \"club\", \"team\", \"transfer\", \"season\", \"competition\", \"matchday\", \"opponent\", \"date\", \"plays for\"]\n",
        "    if any(k in user_input for k in player_keywords):\n",
        "        if qa_pipeline_player and combined_context:\n",
        "             try:\n",
        "                 # Use the 'qa_pipeline_player' pipeline with combined context\n",
        "                 result = qa_pipeline_player(question=user_input, context=combined_context)\n",
        "                 # Check if the answer is likely valid\n",
        "                 if result and result.get('answer') and len(result['answer'].split()) > 1 and result.get('score', 0) > 0.1: # Add a score threshold\n",
        "                     return f\"🤖 Answer: {result['answer']}\"\n",
        "                 else:\n",
        "                     # Fallback if QA doesn't find a good answer in the combined context\n",
        "                     print(\"QA pipeline did not find a confident answer in combined context. Checking APIs...\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Error during QA pipeline processing: {e}\")\n",
        "                 return \"⚠️ An error occurred while trying to answer your question.\"\n",
        "        else:\n",
        "            print(\"QA pipeline not loaded or no dataframes loaded for context. Checking APIs...\")\n",
        "\n",
        "\n",
        "    # Use API if question is about transfers (and QA didn't find a good answer)\n",
        "    if \"transfer\" in user_input or \"signed\" in user_input or \"latest news\" in user_input:\n",
        "        return get_transfer_news()\n",
        "\n",
        "    # Use API if question is about next match (and QA didn't find a good answer)\n",
        "    elif any(kw in user_input for kw in [\"next match\", \"upcoming match\", \"who is playing\", \"next premier league game\"]):\n",
        "        return get_next_match()\n",
        "\n",
        "\n",
        "    # Catch-all fallback if no specific intent matched or QA/API failed\n",
        "    return \"⚽ I'm still learning. Try asking about a player, transfer, or upcoming match!\"\n",
        "\n",
        "print(\"✅ Defined sports_ai_response and API helper functions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Could not load question-answering pipeline: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\n",
            "cannot import name 'convert_batch_encoding' from 'transformers.tf_utils' (/usr/local/lib/python3.11/dist-packages/transformers/tf_utils.py)\n",
            "✅ Defined sports_ai_response and API helper functions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e45be92f",
        "outputId": "3257b51d-de63-4d26-bddf-df88c5a21f56"
      },
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd # Import pandas for DataFrame handling\n",
        "from datetime import datetime # Import datetime to get the current year\n",
        "\n",
        "# API keys (Replace with secure handling in deployment)\n",
        "# It's recommended to use environment variables or Colab secrets for API keys\n",
        "news_api_key = \"b311a02382fa4a88b9d1b4bfc74bb051\"\n",
        "football_api_key = \"5e8310b5845626994bcbf672a6ff5b60\"\n",
        "\n",
        "# Load the question-answering pipeline model (used for player queries)\n",
        "try:\n",
        "    # Note: This uses a different model (distilbert-base-cased-distilled-squad)\n",
        "    # than the FLAN-T5 model loaded earlier.\n",
        "    # Changed model to deepset/roberta-base-squad2 for better compatibility\n",
        "    qa_pipeline_player = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "    print(\"✅ Loaded question-answering pipeline for player queries successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not load question-answering pipeline: {e}\")\n",
        "    qa_pipeline_player = None\n",
        "\n",
        "\n",
        "def get_transfer_news():\n",
        "    url = f\"https://newsapi.org/v2/everything?q=football transfers&language=en&sortBy=publishedAt&apiKey={news_api_key}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"articles\"):\n",
        "            article = data[\"articles\"][0]\n",
        "            return f\"📰 Latest Transfer: \\\"{article['title']}\\\" (Source: {article['source']['name']})\"\n",
        "        return \"⚠️ No transfer news available.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching transfer news: {e}\"\n",
        "\n",
        "\n",
        "def get_next_match():\n",
        "    headers = {\n",
        "        \"x-rapidapi-host\": \"v3.football.api-sports.io\",\n",
        "        \"x-apisports-key\": football_api_key,\n",
        "    }\n",
        "    # Get the current year\n",
        "    current_year = datetime.now().year\n",
        "    # Construct the season string (e.g., \"2023\" or \"2023-2024\")\n",
        "    # For simplicity, let's try the current year first.\n",
        "    # If that doesn't work, we might need to adjust based on the API's expected format (e.g., \"YYYY-YYYY\")\n",
        "    season = str(current_year)\n",
        "    # You might need to adjust the league ID (39 is Premier League) based on what matches you want\n",
        "    url = f\"https://v3.football.api-sports.io/fixtures?league=39&season={season}&next=1\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"response\") and data[\"response\"]: # Check if response list is not empty\n",
        "            match = data[\"response\"][0][\"teams\"]\n",
        "            home_team = match.get('home', {}).get('name', 'Unknown Home Team')\n",
        "            away_team = match.get('away', {}).get('name', 'Unknown Away Team')\n",
        "            return f\"⚽ Next Match: {home_team} vs {away_team}\"\n",
        "        return \"⚠️ No upcoming matches found for the current season or league.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching next match data: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ An unexpected error occurred while processing next match data: {e}\"\n",
        "\n",
        "\n",
        "# AI Response function\n",
        "def sports_ai_response(user_input):\n",
        "    user_input = user_input.lower()\n",
        "\n",
        "    # Combine all loaded dataframes into a single context string for the QA pipeline\n",
        "    combined_context = \"\"\n",
        "    if 'df' in globals() and isinstance(df, pd.DataFrame):\n",
        "        combined_context += df.to_string() + \"\\n\\n\"\n",
        "    if 'df_top250' in globals() and isinstance(df_top250, pd.DataFrame):\n",
        "        combined_context += df_top250.to_string() + \"\\n\\n\"\n",
        "    if 'df_fifa' in globals() and isinstance(df_fifa, pd.DataFrame):\n",
        "        combined_context += df_fifa.to_string() + \"\\n\\n\"\n",
        "    if 'df_data' in globals() and isinstance(df_data, pd.DataFrame):\n",
        "        combined_context += df_data.to_string() + \"\\n\\n\"\n",
        "    if 'df_dataset' in globals() and isinstance(df_dataset, pd.DataFrame):\n",
        "        combined_context += df_dataset.to_string() + \"\\n\\n\"\n",
        "\n",
        "\n",
        "    # Use the question-answering pipeline for factual player-based questions\n",
        "    # Check for keywords that indicate a player-specific question\n",
        "    player_keywords = [\"who\", \"what\", \"how\", \"when\", \"is\", \"was\", \"paid\", \"salary\", \"fee\", \"rating\", \"position\", \"nationality\", \"age\", \"height\", \"weight\", \"club\", \"team\", \"transfer\", \"season\", \"competition\", \"matchday\", \"opponent\", \"date\", \"plays for\"]\n",
        "    if any(k in user_input for k in player_keywords):\n",
        "        if qa_pipeline_player and combined_context:\n",
        "             try:\n",
        "                 # Use the 'qa_pipeline_player' pipeline with combined context\n",
        "                 result = qa_pipeline_player(question=user_input, context=combined_context)\n",
        "                 # Check if the answer is likely valid\n",
        "                 if result and result.get('answer') and len(result['answer'].split()) > 1 and result.get('score', 0) > 0.1: # Add a score threshold\n",
        "                     return f\"🤖 Answer: {result['answer']}\"\n",
        "                 else:\n",
        "                     # Fallback if QA doesn't find a good answer in the combined context\n",
        "                     print(\"QA pipeline did not find a confident answer in combined context. Checking APIs...\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Error during QA pipeline processing: {e}\")\n",
        "                 return \"⚠️ An error occurred while trying to answer your question.\"\n",
        "        else:\n",
        "            print(\"QA pipeline not loaded or no dataframes loaded for context. Checking APIs...\")\n",
        "\n",
        "\n",
        "    # Use API if question is about transfers (and QA didn't find a good answer)\n",
        "    if \"transfer\" in user_input or \"signed\" in user_input or \"latest news\" in user_input:\n",
        "        return get_transfer_news()\n",
        "\n",
        "    # Use API if question is about next match (and QA didn't find a good answer)\n",
        "    elif any(kw in user_input for kw in [\"next match\", \"upcoming match\", \"who is playing\", \"next premier league game\"]):\n",
        "        return get_next_match()\n",
        "\n",
        "\n",
        "    # Catch-all fallback if no specific intent matched or QA/API failed\n",
        "    return \"⚽ I'm still learning. Try asking about a player, transfer, or upcoming match!\"\n",
        "\n",
        "print(\"✅ Defined sports_ai_response and API helper functions.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Could not load question-answering pipeline: Failed to import transformers.models.roberta.modeling_roberta because of the following error (look up to see its traceback):\n",
            "cannot import name 'is_torch_greater_or_equal_than_2_6' from 'transformers.pytorch_utils' (/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py)\n",
            "✅ Defined sports_ai_response and API helper functions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68bbf7d0",
        "outputId": "3312fde2-5fac-4b63-b11f-414fe4731d6c"
      },
      "source": [
        "# Uninstall existing transformers and torch installations\n",
        "!pip uninstall transformers -y -q\n",
        "!pip uninstall torch -y -q\n",
        "\n",
        "# Reinstall transformers and torch to resolve dependencies\n",
        "!pip install transformers torch --no-cache-dir --upgrade -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m187.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m224.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m181.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m186.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m261.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m198.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m228.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m285.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m207.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m153.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m208.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m223.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m268.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m241.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m225.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cbc4a70",
        "outputId": "73198b41-811c-477c-a657-1610d31b9a48"
      },
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd # Import pandas for DataFrame handling\n",
        "\n",
        "# API keys (Replace with secure handling in deployment)\n",
        "# It's recommended to use environment variables or Colab secrets for API keys\n",
        "news_api_key = \"b311a02382fa4a88b9d1b4bfc74bb051\"\n",
        "football_api_key = \"5e8310b5845626994bcbf672a6ff5b60\"\n",
        "\n",
        "# Load the question-answering pipeline model (used for player queries)\n",
        "try:\n",
        "    # Note: This uses a different model (distilbert-base-cased-distilled-squad)\n",
        "    # than the FLAN-T5 model loaded earlier.\n",
        "    # Changed model to deepset/roberta-base-squad2 for better compatibility\n",
        "    qa_pipeline_player = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "    print(\"✅ Loaded question-answering pipeline for player queries successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not load question-answering pipeline: {e}\")\n",
        "    qa_pipeline_player = None\n",
        "\n",
        "\n",
        "def get_transfer_news():\n",
        "    url = f\"https://newsapi.org/v2/everything?q=football transfers&language=en&sortBy=publishedAt&apiKey={news_api_key}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"articles\"):\n",
        "            article = data[\"articles\"][0]\n",
        "            return f\"📰 Latest Transfer: \\\"{article['title']}\\\" (Source: {article['source']['name']})\"\n",
        "        return \"⚠️ No transfer news available.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching transfer news: {e}\"\n",
        "\n",
        "\n",
        "def get_next_match():\n",
        "    headers = {\n",
        "        \"x-rapidapi-host\": \"v3.football.api-sports.io\",\n",
        "        \"x-apisports-key\": football_api_key,\n",
        "    }\n",
        "    # Changed season to 2023 as per API error suggestion for free plan\n",
        "    url = \"https://v3.football.api-sports.io/fixtures?league=39&season=2023&next=1\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"response\"):\n",
        "            match = data[\"response\"][0][\"teams\"]\n",
        "            return f\"⚽ Next Match: {match['home']['name']} vs {match['away']['name']}\"\n",
        "        return \"⚠️ No upcoming matches.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching next match data: {e}\"\n",
        "\n",
        "\n",
        "# AI Response function\n",
        "def sports_ai_response(user_input):\n",
        "    user_input = user_input.lower()\n",
        "\n",
        "    # Combine all loaded dataframes into a single context string for the QA pipeline\n",
        "    combined_context = \"\"\n",
        "    if 'df' in globals() and isinstance(df, pd.DataFrame):\n",
        "        combined_context += df.to_string() + \"\\n\\n\"\n",
        "    if 'df_top250' in globals() and isinstance(df_top250, pd.DataFrame):\n",
        "        combined_context += df_top250.to_string() + \"\\n\\n\"\n",
        "    if 'df_fifa' in globals() and isinstance(df_fifa, pd.DataFrame):\n",
        "        combined_context += df_fifa.to_string() + \"\\n\\n\"\n",
        "    if 'df_data' in globals() and isinstance(df_data, pd.DataFrame):\n",
        "        combined_context += df_data.to_string() + \"\\n\\n\"\n",
        "    if 'df_dataset' in globals() and isinstance(df_dataset, pd.DataFrame):\n",
        "        combined_context += df_dataset.to_string() + \"\\n\\n\"\n",
        "\n",
        "\n",
        "    # Use the question-answering pipeline for factual player-based questions\n",
        "    # Check for keywords that indicate a player-specific question\n",
        "    player_keywords = [\"who\", \"what\", \"how\", \"when\", \"is\", \"was\", \"paid\", \"salary\", \"fee\", \"rating\", \"position\", \"nationality\", \"age\", \"height\", \"weight\", \"club\", \"team\", \"transfer\", \"season\", \"competition\", \"matchday\", \"opponent\", \"date\", \"plays for\"]\n",
        "    if any(k in user_input for k in player_keywords):\n",
        "        if qa_pipeline_player and combined_context:\n",
        "             try:\n",
        "                 # Use the 'qa_pipeline_player' pipeline with combined context\n",
        "                 result = qa_pipeline_player(question=user_input, context=combined_context)\n",
        "                 # Check if the answer is likely valid\n",
        "                 if result and result.get('answer') and len(result['answer'].split()) > 1 and result.get('score', 0) > 0.1: # Add a score threshold\n",
        "                     return f\"🤖 Answer: {result['answer']}\"\n",
        "                 else:\n",
        "                     # Fallback if QA doesn't find a good answer in the combined context\n",
        "                     print(\"QA pipeline did not find a confident answer in combined context. Checking APIs...\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Error during QA pipeline processing: {e}\")\n",
        "                 return \"⚠️ An error occurred while trying to answer your question.\"\n",
        "        else:\n",
        "            print(\"QA pipeline not loaded or no dataframes loaded for context. Checking APIs...\")\n",
        "\n",
        "\n",
        "    # Use API if question is about transfers (and QA didn't find a good answer)\n",
        "    if \"transfer\" in user_input or \"signed\" in user_input or \"latest news\" in user_input:\n",
        "        return get_transfer_news()\n",
        "\n",
        "    # Use API if question is about next match (and QA didn't find a good answer)\n",
        "    elif any(kw in user_input for kw in [\"next match\", \"upcoming match\", \"who is playing\", \"next premier league game\"]):\n",
        "        return get_next_match()\n",
        "\n",
        "\n",
        "    # Catch-all fallback if no specific intent matched or QA/API failed\n",
        "    return \"⚽ I'm still learning. Try asking about a player, transfer, or upcoming match!\"\n",
        "\n",
        "print(\"✅ Defined sports_ai_response and API helper functions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Could not load question-answering pipeline: Failed to import transformers.models.roberta.modeling_roberta because of the following error (look up to see its traceback):\n",
            "cannot import name 'is_torch_greater_or_equal_than_2_6' from 'transformers.pytorch_utils' (/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py)\n",
            "✅ Defined sports_ai_response and API helper functions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "19910716",
        "outputId": "d99ea6f4-84b8-4aad-901d-43bc764e3d8c"
      },
      "source": [
        "!pip install gradio --quiet\n",
        "import gradio as gr\n",
        "\n",
        "def chatbot_interface(message):\n",
        "    # Ensure dataframes are loaded if they are used as context in sports_ai_response\n",
        "    # This might be redundant if the loading cells are guaranteed to run before this,\n",
        "    # but adds robustness if the execution order is not strictly controlled.\n",
        "    global df, df_top250, df_fifa, df_data, df_dataset\n",
        "    # You might need to re-run the data loading cell or ensure persistence\n",
        "\n",
        "    return sports_ai_response(message)\n",
        "\n",
        "demo = gr.Interface(fn=chatbot_interface,\n",
        "                    inputs=\"text\",\n",
        "                    outputs=\"text\",\n",
        "                    title=\"⚽ Sports AI Chatbot\",\n",
        "                    description=\"Ask about football players, transfers, or next matches!\")\n",
        "\n",
        "# To run in Colab, use share=True to get a public URL\n",
        "# For deployment on platforms like Render, share=False is typical,\n",
        "# and the platform handles exposing the service.\n",
        "demo.launch(share=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f6455e06acce319ac0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f6455e06acce319ac0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf46e070",
        "outputId": "be913978-ab77-44af-e4bb-7476bc90bc12"
      },
      "source": [
        "# Attempt to install compatible versions of transformers, torch, and tensorflow\n",
        "# This combination has been found to be more stable for these libraries\n",
        "!pip uninstall transformers torch tensorflow -y -q\n",
        "!pip install transformers==4.30.2 torch==2.0.1+cu118 tensorflow==2.13.0 --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
        "\n",
        "print(\"✅ Attempted to install compatible versions of transformers, torch, and tensorflow.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.30.2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==4.30.2\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Attempted to install compatible versions of transformers, torch, and tensorflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a91029f",
        "outputId": "ba4f8217-3cc3-4d0a-a18c-17c216a6cb94"
      },
      "source": [
        "# Uninstall existing transformers, torch, and tensorflow installations\n",
        "!pip uninstall transformers torch tensorflow -y -q\n",
        "\n",
        "# Reinstall transformers and tensorflow to let pip handle dependencies\n",
        "!pip install transformers tensorflow --no-cache-dir --upgrade -q\n",
        "\n",
        "print(\"✅ Attempted to reinstall transformers and tensorflow.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m195.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m255.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires torch>=1.11.0, which is not installed.\n",
            "peft 0.16.0 requires torch>=1.13.0, which is not installed.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Attempted to reinstall transformers and tensorflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c1d9eda",
        "outputId": "65b39739-8746-4e82-959a-56bd3939c163"
      },
      "source": [
        "# Uninstall existing transformers, torch, and tensorflow installations\n",
        "!pip uninstall transformers torch tensorflow -y -q\n",
        "\n",
        "# Reinstall transformers, torch, and tensorflow to let pip handle dependencies\n",
        "!pip install transformers torch tensorflow --no-cache-dir --upgrade -q\n",
        "\n",
        "print(\"✅ Attempted to reinstall transformers, torch, and tensorflow.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m231.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m182.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Attempted to reinstall transformers, torch, and tensorflow.\n"
          ]
        }
      ]
    }
  ]
}