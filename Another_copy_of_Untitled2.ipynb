{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joeky00/ink-bot-react/blob/main/Another_copy_of_Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0accac26"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the combined QA dataset from the CSV file and apply the preprocessing function to it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0eb94a8"
      },
      "source": [
        "# Cleaned-up Sports AI Chatbot Notebook\n",
        "\n",
        "This notebook contains the essential code to run the Sports AI Chatbot, including data loading, QA generation, model setup, API integration, and the Gradio interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed485e71"
      },
      "source": [
        "## 1. Extract Data from Zip Files\n",
        "\n",
        "Extract the necessary CSV files from the uploaded zip archives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8c43883"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import glob # Import glob to find files\n",
        "\n",
        "# Use glob to find zip files in the /content directory\n",
        "zip_files = glob.glob('/content/archive(*).zip')\n",
        "\n",
        "extract_dir = '/content/'\n",
        "\n",
        "for zip_file_path in zip_files:\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(f\"Successfully extracted {zip_file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {zip_file_path} not found.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: {zip_file_path} is a bad zip file.\")\n",
        "\n",
        "# List files in /content after extraction to confirm\n",
        "print(\"\\nFiles in /content after extraction:\")\n",
        "for file in os.listdir(extract_dir):\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80dbe7c4"
      },
      "source": [
        "## 2. Load DataFrames\n",
        "\n",
        "Load the extracted CSV files into pandas DataFrames. These will be used for generating QA pairs and as context for the AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ecc7275d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load football_players.csv (original dataset)\n",
        "try:\n",
        "    df = pd.read_csv('/content/football_players.csv')\n",
        "    print(\"Loaded football_players.csv\")\n",
        "    # display(df.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: football_players.csv not found.\")\n",
        "    df = None # Ensure df is None if file not found\n",
        "\n",
        "# Load and display top250-00-19.csv\n",
        "try:\n",
        "    df_top250 = pd.read_csv('/content/top250-00-19.csv')\n",
        "    print(\"Loaded top250-00-19.csv\")\n",
        "    # display(df_top250.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: top250-00-19.csv not found.\")\n",
        "    df_top250 = None # Ensure df_top250 is None if file not found\n",
        "\n",
        "# Load and display fifa_players.csv\n",
        "try:\n",
        "    df_fifa = pd.read_csv('/content/fifa_players.csv')\n",
        "    print(\"Loaded fifa_players.csv\")\n",
        "    # display(df_fifa.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: fifa_players.csv not found.\")\n",
        "    df_fifa = None # Ensure df_fifa is None if file not found\n",
        "\n",
        "# Load and display data.csv\n",
        "try:\n",
        "    df_data = pd.read_csv('/content/data.csv')\n",
        "    print(\"Loaded data.csv\")\n",
        "    # display(df_data.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: data.csv not found.\")\n",
        "    df_data = None # Ensure df_data is None if file not found\n",
        "\n",
        "# Load and display dataset.csv\n",
        "try:\n",
        "    df_dataset = pd.read_csv('/content/dataset.csv')\n",
        "    print(\"Loaded dataset.csv\")\n",
        "    # display(df_dataset.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: dataset.csv not found.\")\n",
        "    df_dataset = None # Ensure df_dataset is None if file not found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f8c0a40"
      },
      "source": [
        "## 3. Generate and Combine QA Pairs\n",
        "\n",
        "Define functions to generate QA pairs from each DataFrame and combine them into a single dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a52ea79"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Original QA generation function (assuming df is loaded)\n",
        "def generate_qa_pairs(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"Original df is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"Player\", \"Unknown\")\n",
        "        nationality = player.get(\"Origin\", \"Unknown\")\n",
        "        club = player.get(\"To(Club)\", \"Unknown\")\n",
        "        position = player.get(\"Position\", \"Unknown\")\n",
        "        fee = player.get(\"Fee(€ mln)\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club does {name} play for?\",\n",
        "            f\"What is the transfer fee for {name} in € mln?\",\n",
        "            f\"What position does {name} play?\",\n",
        "            f\"What is the nationality of {name}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{name} plays for {club}.\",\n",
        "            f\"The transfer fee for {name} was {fee} € mln.\",\n",
        "            f\"{name} plays as a {position}.\",\n",
        "            f\"{name} is from {nationality}.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_top250\n",
        "def generate_qa_pairs_top250(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_top250 is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"Name\", \"Unknown\")\n",
        "        position = player.get(\"Position\", \"Unknown\")\n",
        "        team_from = player.get(\"Team_from\", \"Unknown\")\n",
        "        team_to = player.get(\"Team_to\", \"Unknown\")\n",
        "        transfer_fee = player.get(\"Transfer_fee\", \"Unknown\")\n",
        "        season = player.get(\"Season\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club did {name} transfer to in the {season} season?\",\n",
        "            f\"What was the transfer fee for {name} in the {season} season?\",\n",
        "            f\"What position did {name} play?\",\n",
        "            f\"Which club did {name} transfer from in the {season} season?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{name} transferred to {team_to} in the {season} season.\",\n",
        "            f\"The transfer fee for {name} in the {season} season was {transfer_fee}.\",\n",
        "            f\"{name} played as a {position}.\",\n",
        "            f\"{name} transferred from {team_from} in the {season} season.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_fifa\n",
        "def generate_qa_pairs_fifa(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_fifa is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player = df.sample(1).iloc[0]\n",
        "        name = player.get(\"name\", \"Unknown\")\n",
        "        full_name = player.get(\"full_name\", \"Unknown\")\n",
        "        nationality = player.get(\"nationality\", \"Unknown\")\n",
        "        overall_rating = player.get(\"overall_rating\", \"Unknown\")\n",
        "        age = player.get(\"age\", \"Unknown\")\n",
        "        positions = player.get(\"positions\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"What is the full name of {name}?\",\n",
        "            f\"What is the overall rating of {name}?\",\n",
        "            f\"What is the nationality of {name}?\",\n",
        "            f\"What is the age of {name}?\",\n",
        "            f\"What positions does {name} play?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"The full name of {name} is {full_name}.\",\n",
        "            f\"The overall rating of {name} is {overall_rating}.\",\n",
        "            f\"{name} is from {nationality}.\",\n",
        "            f\"{name} is {age} years old.\",\n",
        "            f\"{name} plays in the {positions} positions.\",\n",
        "        ]\n",
        "\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_data\n",
        "def generate_qa_pairs_data(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_data is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        match_event = df.sample(1).iloc[0]\n",
        "        player = match_event.get(\"Player\", \"Unknown\")\n",
        "        season = match_event.get(\"Season\", \"Unknown\")\n",
        "        competition = match_event.get(\"Competition\", \"Unknown\")\n",
        "        club = match_event.get(\"Club\", \"Unknown\")\n",
        "        opponent = match_event.get(\"Opponent\", \"Unknown\")\n",
        "        matchday = match_event.get(\"Matchday\", \"Unknown\")\n",
        "        date = match_event.get(\"Date\", \"Unknown\")\n",
        "\n",
        "\n",
        "        questions = [\n",
        "            f\"What competition did {player} play in during the {season} season on matchday {matchday}?\",\n",
        "            f\"Which club did {player} play for in the match against {opponent} on {date}?\",\n",
        "            f\"What was the opponent when {player} played for {club} on {date}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{player} played in the {competition} competition during the {season} season on matchday {matchday}.\",\n",
        "            f\"{player} played for {club} in the match against {opponent} on {date}.\",\n",
        "            f\"The opponent was {opponent} when {player} played for {club} on {date}.\",\n",
        "        ]\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "# QA generation function for df_dataset\n",
        "def generate_qa_pairs_dataset(df, num_samples=10):\n",
        "    qa_pairs = []\n",
        "    if df is None:\n",
        "        print(\"df_dataset is not loaded for QA generation.\")\n",
        "        return qa_pairs\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        player_transfer = df.sample(1).iloc[0]\n",
        "        player = player_transfer.get(\"Player\", \"Unknown\")\n",
        "        year = player_transfer.get(\"Year\", \"Unknown\")\n",
        "        fee = player_transfer.get(\"Fee\", \"Unknown\")\n",
        "        from_club = player_transfer.get(\"From Club\", \"Unknown\")\n",
        "        to_club = player_transfer.get(\"To Club\", \"Unknown\")\n",
        "\n",
        "        questions = [\n",
        "            f\"Which club did {player} transfer to in {year}?\",\n",
        "            f\"What was the transfer fee for {player} in {year}?\",\n",
        "            f\"Which club did {player} transfer from in {year}?\",\n",
        "            f\"In what year did {player} transfer from {from_club} to {to_club}?\",\n",
        "        ]\n",
        "\n",
        "        answers = [\n",
        "            f\"{player} transferred to {to_club} in {year}.\",\n",
        "            f\"The transfer fee for {player} in {year} was {fee}.\",\n",
        "            f\"{player} transferred from {from_club} in {year}.\",\n",
        "            f\"{player} transferred from {from_club} to {to_club} in {year}.\",\n",
        "        ]\n",
        "        qa_pairs.extend(list(zip(questions, answers)))\n",
        "\n",
        "    return qa_pairs\n",
        "\n",
        "\n",
        "# Generate QA pairs from each dataframe that was successfully loaded\n",
        "all_qa_data = []\n",
        "if df is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs(df, num_samples=50)) # Increased samples for more data\n",
        "if df_top250 is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_top250(df_top250, num_samples=50)) # Increased samples\n",
        "if df_fifa is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_fifa(df_fifa, num_samples=50)) # Increased samples\n",
        "if df_data is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_data(df_data, num_samples=50)) # Increased samples\n",
        "if df_dataset is not None:\n",
        "    all_qa_data.extend(generate_qa_pairs_dataset(df_dataset, num_samples=50)) # Increased samples\n",
        "\n",
        "\n",
        "# Save the combined QA pairs into a CSV file\n",
        "with open(\"combined_qa_dataset.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"question\", \"answer\"])\n",
        "    writer.writerows(all_qa_data)\n",
        "\n",
        "print(f\"✅ Saved combined QA dataset with {len(all_qa_data)} pairs as combined_qa_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eadd7cc"
      },
      "source": [
        "## 4. Load Model and Tokenizer\n",
        "\n",
        "Load the FLAN-T5 model and tokenizer for text generation (and potentially for fine-tuning if needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d41a97e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "print(\"✅ Loaded flan-t5-small model successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06f42041"
      },
      "source": [
        "## 5. Preprocess the Combined Dataset\n",
        "\n",
        "Load the combined QA dataset and preprocess it for potential use with a language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ba7749"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the combined dataset from the CSV file\n",
        "combined_dataset = load_dataset('csv', data_files='combined_qa_dataset.csv')\n",
        "\n",
        "# Load tokenizer (ensure it's loaded if session restarted)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\") # Assuming tokenizer is already loaded\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(example):\n",
        "    input_text = \"question: \" + example[\"question\"]\n",
        "    target_text = example[\"answer\"]\n",
        "    tokenized_input = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=128)\n",
        "    tokenized_target = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=32)\n",
        "\n",
        "    tokenized_input[\"labels\"] = tokenized_target[\"input_ids\"]\n",
        "    return tokenized_input\n",
        "\n",
        "# Apply preprocessing to the combined dataset\n",
        "tokenized_combined_dataset = combined_dataset.map(preprocess)\n",
        "\n",
        "# Preview tokenized combined data\n",
        "print(\"Preview of tokenized combined dataset:\")\n",
        "print(tokenized_combined_dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8f4f8e2"
      },
      "source": [
        "## 6. Define API and AI Response Functions\n",
        "\n",
        "Define the functions to interact with external APIs for news and matches, and the main AI response function that routes user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5fe0cb8"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd # Import pandas for DataFrame handling\n",
        "\n",
        "# API keys (Replace with secure handling in deployment)\n",
        "# It's recommended to use environment variables or Colab secrets for API keys\n",
        "news_api_key = \"b311a02382fa4a88b9d1b4bfc74bb051\"\n",
        "football_api_key = \"5e8310b5845626994bcbf672a6ff5b60\"\n",
        "\n",
        "# Load the question-answering pipeline model (used for player queries)\n",
        "try:\n",
        "    # Note: This uses a different model (distilbert-base-cased-distilled-squad)\n",
        "    # than the FLAN-T5 model loaded earlier.\n",
        "    qa_pipeline_player = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "    print(\"✅ Loaded question-answering pipeline for player queries successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not load question-answering pipeline: {e}\")\n",
        "    qa_pipeline_player = None\n",
        "\n",
        "\n",
        "def get_transfer_news():\n",
        "    url = f\"https://newsapi.org/v2/everything?q=football transfers&language=en&sortBy=publishedAt&apiKey={news_api_key}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"articles\"):\n",
        "            article = data[\"articles\"][0]\n",
        "            return f\"📰 Latest Transfer: \\\"{article['title']}\\\" (Source: {article['source']['name']})\"\n",
        "        return \"⚠️ No transfer news available.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching transfer news: {e}\"\n",
        "\n",
        "\n",
        "def get_next_match():\n",
        "    headers = {\n",
        "        \"x-rapidapi-host\": \"v3.football.api-sports.io\",\n",
        "        \"x-apisports-key\": football_api_key,\n",
        "    }\n",
        "    # Changed season to 2023 as per API error suggestion for free plan\n",
        "    url = \"https://v3.football.api-sports.io/fixtures?league=39&season=2023&next=1\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        if data.get(\"response\"):\n",
        "            match = data[\"response\"][0][\"teams\"]\n",
        "            return f\"⚽ Next Match: {match['home']['name']} vs {match['away']['name']}\"\n",
        "        return \"⚠️ No upcoming matches.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"⚠️ Error fetching next match data: {e}\"\n",
        "\n",
        "\n",
        "# AI Response function\n",
        "def sports_ai_response(user_input):\n",
        "    user_input = user_input.lower()\n",
        "\n",
        "    # Combine all loaded dataframes into a single context string for the QA pipeline\n",
        "    combined_context = \"\"\n",
        "    if 'df' in globals() and isinstance(df, pd.DataFrame):\n",
        "        combined_context += df.to_string() + \"\\n\\n\"\n",
        "    if 'df_top250' in globals() and isinstance(df_top250, pd.DataFrame):\n",
        "        combined_context += df_top250.to_string() + \"\\n\\n\"\n",
        "    if 'df_fifa' in globals() and isinstance(df_fifa, pd.DataFrame):\n",
        "        combined_context += df_fifa.to_string() + \"\\n\\n\"\n",
        "    if 'df_data' in globals() and isinstance(df_data, pd.DataFrame):\n",
        "        combined_context += df_data.to_string() + \"\\n\\n\"\n",
        "    if 'df_dataset' in globals() and isinstance(df_dataset, pd.DataFrame):\n",
        "        combined_context += df_dataset.to_string() + \"\\n\\n\"\n",
        "\n",
        "\n",
        "    # Use the question-answering pipeline for factual player-based questions\n",
        "    # Check for keywords that indicate a player-specific question\n",
        "    player_keywords = [\"who\", \"what\", \"how\", \"when\", \"is\", \"was\", \"paid\", \"salary\", \"fee\", \"rating\", \"position\", \"nationality\", \"age\", \"height\", \"weight\", \"club\", \"team\", \"transfer\", \"season\", \"competition\", \"matchday\", \"opponent\", \"date\", \"plays for\"]\n",
        "    if any(k in user_input for k in player_keywords):\n",
        "        if qa_pipeline_player and combined_context:\n",
        "             try:\n",
        "                 # Use the 'qa_pipeline_player' pipeline with combined context\n",
        "                 result = qa_pipeline_player(question=user_input, context=combined_context)\n",
        "                 # Check if the answer is likely valid\n",
        "                 if result and result.get('answer') and len(result['answer'].split()) > 1 and result.get('score', 0) > 0.1: # Add a score threshold\n",
        "                     return f\"🤖 Answer: {result['answer']}\"\n",
        "                 else:\n",
        "                     # Fallback if QA doesn't find a good answer in the combined context\n",
        "                     print(\"QA pipeline did not find a confident answer in combined context. Checking APIs...\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Error during QA pipeline processing: {e}\")\n",
        "                 return \"⚠️ An error occurred while trying to answer your question.\"\n",
        "        else:\n",
        "            print(\"QA pipeline not loaded or no dataframes loaded for context. Checking APIs...\")\n",
        "\n",
        "\n",
        "    # Use API if question is about transfers (and QA didn't find a good answer)\n",
        "    if \"transfer\" in user_input or \"signed\" in user_input or \"latest news\" in user_input:\n",
        "        return get_transfer_news()\n",
        "\n",
        "    # Use API if question is about next match (and QA didn't find a good answer)\n",
        "    elif any(kw in user_input for kw in [\"next match\", \"upcoming match\", \"who is playing\", \"next premier league game\"]):\n",
        "        return get_next_match()\n",
        "\n",
        "\n",
        "    # Catch-all fallback if no specific intent matched or QA/API failed\n",
        "    return \"⚽ I'm still learning. Try asking about a player, transfer, or upcoming match!\"\n",
        "\n",
        "print(\"✅ Defined sports_ai_response and API helper functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97af9394"
      },
      "source": [
        "## 7. Launch Gradio Interface\n",
        "\n",
        "Set up and launch the Gradio interface to interact with the chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68fecbad"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --quiet\n",
        "import gradio as gr\n",
        "\n",
        "def chatbot_interface(message):\n",
        "    # Ensure dataframes are loaded if they are used as context in sports_ai_response\n",
        "    # This might be redundant if the loading cells are guaranteed to run before this,\n",
        "    # but adds robustness if the execution order is not strictly controlled.\n",
        "    global df, df_top250, df_fifa, df_data, df_dataset\n",
        "    # You might need to re-run the data loading cell or ensure persistence\n",
        "\n",
        "    return sports_ai_response(message)\n",
        "\n",
        "demo = gr.Interface(fn=chatbot_interface,\n",
        "                    inputs=\"text\",\n",
        "                    outputs=\"text\",\n",
        "                    title=\"⚽ Sports AI Chatbot\",\n",
        "                    description=\"Ask about football players, transfers, or next matches!\")\n",
        "\n",
        "# To run in Colab, use share=True to get a public URL\n",
        "# For deployment on platforms like Render, share=False is typical,\n",
        "# and the platform handles exposing the service.\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0187bd8"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import glob # Import glob to find files\n",
        "\n",
        "# Use glob to find zip files in the /content directory - using a simpler pattern\n",
        "zip_files = glob.glob('/content/*.zip')\n",
        "\n",
        "extract_dir = '/content/'\n",
        "\n",
        "for zip_file_path in zip_files:\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(f\"Successfully extracted {zip_file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {zip_file_path} not found.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: {zip_file_path} is a bad zip file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while processing {zip_file_path}: {e}\")\n",
        "\n",
        "\n",
        "# List files in /content after extraction to confirm\n",
        "print(\"\\nFiles in /content after extraction:\")\n",
        "try:\n",
        "    for file in os.listdir(extract_dir):\n",
        "        print(file)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Directory {extract_dir} not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb974327"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load football_players.csv (original dataset)\n",
        "try:\n",
        "    df = pd.read_csv('/content/football_players.csv')\n",
        "    print(\"Loaded football_players.csv\")\n",
        "    # display(df.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: football_players.csv not found.\")\n",
        "    df = None # Ensure df is None if file not found\n",
        "\n",
        "# Load and display top250-00-19.csv\n",
        "try:\n",
        "    df_top250 = pd.read_csv('/content/top250-00-19.csv')\n",
        "    print(\"Loaded top250-00-19.csv\")\n",
        "    # display(df_top250.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: top250-00-19.csv not found.\")\n",
        "    df_top250 = None # Ensure df_top250 is None if file not found\n",
        "\n",
        "# Load and display fifa_players.csv\n",
        "try:\n",
        "    df_fifa = pd.read_csv('/content/fifa_players.csv')\n",
        "    print(\"Loaded fifa_players.csv\")\n",
        "    # display(df_fifa.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: fifa_players.csv not found.\")\n",
        "    df_fifa = None # Ensure df_fifa is None if file not found\n",
        "\n",
        "# Load and display data.csv\n",
        "try:\n",
        "    df_data = pd.read_csv('/content/data.csv')\n",
        "    print(\"Loaded data.csv\")\n",
        "    # display(df_data.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: data.csv not found.\")\n",
        "    df_data = None # Ensure df_data is None if file not found\n",
        "\n",
        "# Load and display dataset.csv\n",
        "try:\n",
        "    df_dataset = pd.read_csv('/content/dataset.csv')\n",
        "    print(\"Loaded dataset.csv\")\n",
        "    # display(df_dataset.head()) # Optional: display head for verification\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: dataset.csv not found.\")\n",
        "    df_dataset = None # Ensure df_dataset is None if file not found"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWX3AG5rhf9Du5+FM2kfDJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}